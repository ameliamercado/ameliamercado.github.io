---
title: 'project2'
output: html_document
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="importing-data-loading-librariesfunctions" class="section level2">
<h2>Importing Data + Loading libraries/functions</h2>
<pre class="r"><code>library(tidyverse); library(glmnet); library(rstatix); library(sandwich); library(lmtest);library(plotROC); library(interactions)</code></pre>
<pre><code>## ── Attaching packages ────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.3     ✓ dplyr   1.0.1
## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.5.0</code></pre>
<pre><code>## ── Conflicts ───────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, pack, unpack</code></pre>
<pre><code>## Loaded glmnet 4.0-2</code></pre>
<pre><code>## 
## Attaching package: &#39;rstatix&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     filter</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre class="r"><code>data &lt;- read.csv(&quot;/Users/ameliamercado/Desktop/website/content/project/heart.csv&quot;)  %&gt;% rename(max=thalach) %&gt;% mutate(sex=as.factor(sex)) %&gt;% mutate(fbs=as.factor(fbs)) %&gt;% mutate(exang=as.factor(exang)) %&gt;% mutate(cp=as.factor(cp)) %&gt;% mutate(slope=as.factor(slope)) %&gt;% mutate(ca=as.factor(ca)) %&gt;% mutate(thal=as.factor(thal)) %&gt;% mutate(target=as.factor(target))
class_diag&lt;-function(probs,truth){
  tab&lt;-table(factor(probs&gt;.5,levels=c(&quot;FALSE&quot;,&quot;TRUE&quot;)),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]
  f1=2*(sens*ppv)/(sens+ppv)

  if(is.numeric(truth)==FALSE &amp; is.logical(truth)==FALSE){
    truth&lt;-as.numeric(truth)-1}
  
  #CALCULATE EXACT AUC
  ord&lt;-order(probs, decreasing=TRUE)
  probs &lt;- probs[ord]; truth &lt;- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup&lt;-c(probs[-1]&gt;=probs[-length(probs)], FALSE)
  TPR&lt;-c(0,TPR[!dup],1); FPR&lt;-c(0,FPR[!dup],1)
  
  n &lt;- length(TPR)
  auc&lt;- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,f1,auc)
}</code></pre>
</div>
<div id="data" class="section level2">
<h2>0. Data</h2>
<pre class="r"><code>head(data)  </code></pre>
<pre><code>##   age sex cp trestbps chol fbs restecg max exang oldpeak slope ca thal target
## 1  63   1  3      145  233   1       0 150     0     2.3     0  0    1      1
## 2  37   1  2      130  250   0       1 187     0     3.5     0  0    2      1
## 3  41   0  1      130  204   0       0 172     0     1.4     2  0    2      1
## 4  56   1  1      120  236   0       1 178     0     0.8     2  0    2      1
## 5  57   0  0      120  354   0       1 163     1     0.6     2  0    2      1
## 6  57   1  0      140  192   0       1 148     0     0.4     1  0    1      1</code></pre>
<pre class="r"><code>nrow(data)</code></pre>
<pre><code>## [1] 303</code></pre>
<p>This dataset was pulled from the Cleveland Heart Disease dataset from the UCI repository. Each row represents a patient and some metrics/demographic information associated with them; the goal of this dataset I believe is to provide information about those who do and do not have heart disease. The <code>age</code> variable represents age in years, <code>trestbps</code> represent resting blood pressure in mmHg, <code>chol</code> represents cholesterol in mg/dl, <code>max</code> represents maximum heart rate during exercise, and <code>oldpeak</code> represents ST depression induced by exercise relative to rest on an ECG. Some of the categorical variables are <code>sex</code>, which represents sex (0=male, 1=female), <code>cp</code>, which represents chest pain type (0=typical angina, 1= atypical angina, 2= nonanginal pain, 3=asymptotic), <code>thal</code>, which represents the results from a thallium test (0=normal, 1=fixed defect, 2=reversible defect), <code>target</code>, which represents those who have heart disease (1=yes, 0=no), and lastly, <code>exang</code>, which represents exercise induced angina (1=yes, 0=no). There are 303 observations.</p>
</div>
<div id="manova" class="section level2">
<h2>1. MANOVA</h2>
<pre class="r"><code>man &lt;- manova(cbind(age, trestbps, chol, max, oldpeak) ~ cp, data=data); summary(man) # 1 test</code></pre>
<pre><code>##            Df  Pillai approx F num Df den Df    Pr(&gt;F)    
## cp          3 0.24915   5.3799     15    891 1.753e-10 ***
## Residuals 299                                             
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>summary.aov(man) #5 tests</code></pre>
<pre><code>##  Response age :
##              Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## cp            3    818 272.674   3.384 0.01856 *
## Residuals   299  24092  80.576                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response trestbps :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)  
## cp            3   2643  881.03  2.9189 0.0344 *
## Residuals   299  90248  301.83                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response chol :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## cp            3   5001  1666.9  0.6181 0.6037
## Residuals   299 806300  2696.7               
## 
##  Response max :
##              Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## cp            3  24030  8009.9  17.818 1.149e-10 ***
## Residuals   299 134413   449.5                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response oldpeak :
##              Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## cp            3  51.00 17.0000  14.273 1.023e-08 ***
## Residuals   299 356.12  1.1911                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>pairwise.t.test(data$age, data$cp, p.adj=&quot;none&quot;) #6 tests</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  data$age and data$cp 
## 
##   0      1      2     
## 1 0.0036 -      -     
## 2 0.0757 0.1767 -     
## 3 0.9300 0.0471 0.2646
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(data$trestbps, data$cp, p.adj=&quot;none&quot;) #6 tests</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  data$trestbps and data$cp 
## 
##   0      1      2     
## 1 0.2056 -      -     
## 2 0.4876 0.5214 -     
## 3 0.0241 0.0047 0.0105
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(data$max, data$cp, p.adj=&quot;none&quot;) #6 tests</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  data$max and data$cp 
## 
##   0       1      2     
## 1 1.2e-09 -      -     
## 2 3.2e-07 0.0713 -     
## 3 0.0013  0.2273 0.9443
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(data$oldpeak, data$cp, p.adj=&quot;none&quot;) #6 tests</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  data$oldpeak and data$cp 
## 
##   0       1       2      
## 1 7.4e-09 -       -      
## 2 9.9e-05 0.01342 -      
## 3 0.97371 0.00011 0.02102
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>1-.95^(30) #type 1 error rate</code></pre>
<pre><code>## [1] 0.7853612</code></pre>
<pre class="r"><code>.05/30 #Bonferroni correction </code></pre>
<pre><code>## [1] 0.001666667</code></pre>
<pre class="r"><code>#Checking MANOVA assumptions 
group &lt;- data$cp
DVs &lt;- data %&gt;% select(age, trestbps, chol, max)
sapply(split(DVs,group), mshapiro_test)</code></pre>
<pre><code>##           0          1            2            3         
## statistic 0.9683318  0.8782083    0.8661527    0.9138454 
## p.value   0.00213957 9.876928e-05 2.397808e-07 0.04925103</code></pre>
<p>Based on our MANOVA, significant differences were found among the three levels of chest pain for at least one of the dependent variables. As a follow-up test, univariate ANOVAs for each dependent variable were conducted. The univariate ANOVAs for <code>age</code> (p=0.02), <code>trestbps</code>(p=0.03), <code>max</code>(p&lt;.001), and <code>oldpeak</code>(p&lt;0.001) were all significant. Post hoc analysis was conducted via pairwise comparisons to determine which levels of <code>cp</code> differed for <code>age</code>, <code>trestbps</code>, <code>max</code>, and <code>oldpeak</code>. We conducted a total of 30 tests, which yielded a type I error rate of 0.785 and a bonferroni adjusted significance of 0.0017.</p>
<p>In post-hoc analysis, for <code>age</code> and <code>trestbps</code> nothing was found to be significant between all <code>cp</code> groups. A <code>cp</code> level of 0 showed a significant mean <code>max</code> difference from a <code>cp</code> level of 1 (p&lt;0.0001) and a level of 2 (p&lt;0.0001). A <code>cp</code> level of 0 showed a significant <code>oldpeak</code> mean difference from a <code>cp</code> level of 1 (p&lt;0.0001) and 2 (p&lt;0.0001).</p>
<p>While checking assumptions for MANOVA, we worked to see if the multivariate normality of DVs assumption was met. We ran a Shapiro-Wilk test to check this, and we had at 4 p-values that were less than .05, which meant we could reject the null hypothesis and the multivariate normality of DVs assumption was not met.</p>
</div>
<div id="randomization-test" class="section level2">
<h2>2. Randomization test</h2>
<pre class="r"><code>set.seed(1234) 
rand &lt;- vector() 
for(i in 1:5000){
new&lt;-data.frame(chol=sample(data$chol),exang=data$exang) 
rand[i]&lt;-mean(new[new$exang==0,]$chol)-   
              mean(new[new$exang==1,]$chol)}  
data%&gt;%group_by(exang)%&gt;%summarize(means=mean(chol))%&gt;%summarize(`mean_diff`=diff(means)) </code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre><code>## # A tibble: 1 x 1
##   mean_diff
##       &lt;dbl&gt;
## 1      7.39</code></pre>
<pre class="r"><code>mean(rand&gt;7.394385| rand&lt; -7.394385)</code></pre>
<pre><code>## [1] 0.2518</code></pre>
<pre class="r"><code>{hist(rand,main=&quot;Null Distribution and Test Statistic&quot;,ylab=&quot;Frequency&quot;, xlab=&quot;rand_exang&quot;); abline(v = c(-7.394385 , 7.394385),col=&quot;red&quot;)}   </code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>t.test(data=data, chol~exang) </code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  chol by exang
## t = -1.1929, df = 206.28, p-value = 0.2343
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -19.615616   4.826846
## sample estimates:
## mean in group 0 mean in group 1 
##        243.8480        251.2424</code></pre>
<p>We decided to perform a permutation test to compare mean cholesterol between those who have exercise induced angina and those who do not. Our null hypothesis was that the mean cholesterol is the same for those who have exercise induced angina vs. those who do not. Our alternative hypothesis was that the mean cholesterol is different for those who have exercise induced angina vs. those who do not. The observed cholesterol difference in means was 7.394 mg/dl, which means that those who had exercise induced angina on average had cholesterol that was 7.3943 higher than those who do not; this value was considered the test statistic.</p>
<p>The probability (p-value) of getting a mean difference as extreme as the one we got under our randomization distribution was 0.2352. Thus, we fail to reject the null hypothesis and cannot state that there is a significant mean difference for cholesterol between those who have exercise induced angina and those who do not. We also created a plot visualizing the null distribution and the test statistic. Our plot is consistent with our findings, considering our test statistic is near the center of the distribution. Lastly, we checked our results from the randomization test with an independent t-test, and arrived at the same conclusion (p=0.234).</p>
</div>
<div id="linear-regression-model" class="section level2">
<h2>3. Linear regression model</h2>
<pre class="r"><code>#Model 
data$age_c &lt;- (data$age -mean(data$age))
fit &lt;- lm(trestbps ~ sex * age_c, data=data)
summary(fit)  </code></pre>
<pre><code>## 
## Call:
## lm(formula = trestbps ~ sex * age_c, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -37.258 -11.062  -0.969  10.270  66.709 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 132.2388     1.7418  75.922  &lt; 2e-16 ***
## sex1         -1.0022     2.1024  -0.477 0.633947    
## age_c         0.6443     0.1843   3.497 0.000543 ***
## sex1:age_c   -0.1677     0.2270  -0.739 0.460643    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 16.9 on 299 degrees of freedom
## Multiple R-squared:  0.08058,    Adjusted R-squared:  0.07136 
## F-statistic: 8.735 on 3 and 299 DF,  p-value: 1.429e-05</code></pre>
<pre class="r"><code>#Plotting the regression  
#Plot 1
data %&gt;% ggplot(aes(age_c, trestbps, color=sex)) +geom_point() +geom_smooth(method=&quot;lm&quot;, se=F)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>#Plot 2 ( I thought both looked cool! )
interact_plot(fit,age_c,sex)</code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<pre class="r"><code>#Checking assumptions (linearity)
resids&lt;-fit$residuals
fitvals&lt;-fit$fitted.values 
data.frame(resids,fitvals)%&gt;%ggplot(aes(fitvals,resids))+geom_point()+geom_hline(yintercept=0, color=&#39;red&#39;)  </code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-5-3.png" width="672" /></p>
<pre class="r"><code>#Checking assumptions(homoskedasticity)
bptest(fit)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  fit
## BP = 9.0701, df = 3, p-value = 0.02837</code></pre>
<pre class="r"><code>#Checking assumptions (normality)
shapiro.test(resids)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resids
## W = 0.97989, p-value = 0.0002932</code></pre>
<pre class="r"><code>#Recomputing regression with robust standard errors
coeftest(fit, vcov = vcovHC(fit))</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 132.23876    1.83819 71.9395  &lt; 2e-16 ***
## sex1         -1.00215    2.16797 -0.4623  0.64424    
## age_c         0.64435    0.19405  3.3206  0.00101 ** 
## sex1:age_c   -0.16770    0.22886 -0.7328  0.46428    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#Adjusting R-squared
(sum((data$trestbps-mean(data$trestbps))^2)-sum(fit$residuals^2))/sum((data$trestbps-mean(data$trestbps))^2) </code></pre>
<pre><code>## [1] 0.08058005</code></pre>
<p>132.2388 is the predicted resting BP for males with an average age. For people with an average age, females have a predicted resting BP that is 1.0022 lower than males. For every 1-unit increase in age, predicted resting BP increases by 0.6443. Slope of age on BP for females is 0.1677 lower than for males.</p>
<p>While checking for assumptions, we first checked for linearity; our plot of residuals vs. fitted values appeared to have some funneling, which meant we did not meet the linearity assumption. We ran a bptest to check for homoskedasticity, and arrived at a p-value of 0.028. The null hypothesis for our bptest was that the data was homoskedastic; because we were able to reject the null hypothesis, we failed to meet the homoskedastic assumption. To check for normality, we ran a Shapiro-Wilk test, which yielded a p-value &lt;0.001. The null hypothesis for our Shapiro-Wilk test was that hte data was normal; because we were able to reject the null hypothesis, we failed to meet the normality assumption.</p>
<p>Next, we recomputed our regression results with robust standard errors by using <code>coeftest.</code> Even after computing for robust SEs, age_c still had a significant effect on resting BP. Our R-squared was 0.08058, and after adjusting, it was was 0.08058 (the same). Thus, 8.06% of the variability in resting BP is explained by our model.</p>
</div>
<div id="bootstrap-ses" class="section level2">
<h2>4. Bootstrap SEs</h2>
<pre class="r"><code>#Model
resids&lt;-fit$residuals
fitted&lt;-fit$fitted.values 
set.seed(1234)
resid_resamp&lt;-replicate(5000,{
  new_resids&lt;-sample(resids,replace=TRUE) 
  data$new_y&lt;-fitted+new_resids 
  fit&lt;-lm(new_y ~ sex * age_c ,data=data) 
  coef(fit) 
})
resid_resamp %&gt;% t %&gt;% as.data.frame %&gt;% summarize_all(sd)</code></pre>
<pre><code>##   (Intercept)     sex1     age_c sex1:age_c
## 1     1.74398 2.118223 0.1817665  0.2248934</code></pre>
<pre class="r"><code>resid_resamp %&gt;% t %&gt;% as.data.frame %&gt;% pivot_longer(1:4) %&gt;% group_by(name) %&gt;% summarize(lower=quantile(value, 0.025), upper=quantile(value, 0.95))  </code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   name          lower   upper
##   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept) 129.    135.   
## 2 age_c         0.280   0.946
## 3 sex1         -5.17    2.42 
## 4 sex1:age_c   -0.609   0.204</code></pre>
<p>Comparing the original SEs to the bootstrap SEs , sex1 changed from 2.10 to 2.12, age_c changed from 0.184 to 0.182, and the interaction changed from 0.227 to 0.225. The differences were thus small. Comparing the robust SEs to the bootstrap SEs, sex1 changed from 2.16 to 2.12, age_c changed from 0.19 to 0.18, and the interaction changed from 0.229 to 0.225. Due to little change in SEs, it is reasonable to conclude that the p-values more or less stayed the same as well. When computing a 95% CI for the bootstrap SEs, age_c was the only significant variable because 0 was not in the CI, which was consistent with the findings from the original and robust SEs.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>5. Logistic regression</h2>
<pre class="r"><code>#Model 
fit2 &lt;- glm(sex~trestbps+chol, data=data, family=&quot;binomial&quot;); summary(fit2); exp(coef(fit2))</code></pre>
<pre><code>## 
## Call:
## glm(formula = sex ~ trestbps + chol, family = &quot;binomial&quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9540  -1.3156   0.7574   0.8803   1.2088  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  3.332833   1.082115   3.080  0.00207 **
## trestbps    -0.004046   0.007157  -0.565  0.57182   
## chol        -0.008137   0.002539  -3.204  0.00135 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 378.42  on 302  degrees of freedom
## Residual deviance: 366.33  on 300  degrees of freedom
## AIC: 372.33
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## (Intercept)    trestbps        chol 
##  28.0175973   0.9959621   0.9918963</code></pre>
<pre class="r"><code>probs&lt;-predict(fit2,type=&quot;response&quot;)   
probs &lt;- as.numeric(probs)   
#Confusion Matrix 
table(predict=as.numeric(probs&gt;.5),truth=data$sex)%&gt;%addmargins </code></pre>
<pre><code>##        truth
## predict   0   1 Sum
##     0     9   2  11
##     1    87 205 292
##     Sum  96 207 303</code></pre>
<pre class="r"><code>#In-sample classification diagnostics
class_diag(probs, data$sex) </code></pre>
<pre><code>##         acc      sens    spec       ppv        f1       auc
## 1 0.7062706 0.9903382 0.09375 0.7020548 0.8216433 0.5970964</code></pre>
<pre class="r"><code>#Density Plot
data$logit&lt;-predict(fit2, type=&quot;link&quot;)
data %&gt;% ggplot(aes(logit, fill=sex))+geom_density(alpha=.3)+
  geom_vline(xintercept=0,lty=2)</code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>#ROC Curve
data$sex2 &lt;- as.numeric(data$sex)
ROCplot&lt;-ggplot(data)+geom_roc(aes(d=sex2,m=probs), n.cuts=0); ROCplot; calc_auc(ROCplot) </code></pre>
<pre><code>## Warning in verify_d(data$d): D not labeled 0/1, assuming 1 = 0 and 2 = 1!</code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<pre><code>## Warning in verify_d(data$d): D not labeled 0/1, assuming 1 = 0 and 2 = 1!</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.5970964</code></pre>
<p>Odds of being female when <code>trestbps</code>=0 and <code>chol</code>=0 is 28.02. While controlling for <code>chol</code>, for every 1-unit increase in <code>trestbps</code>, odds of being female increase by a factor of 0.996 (p=0.57). While controlling for <code>trestbps</code>, for every 1-unit increase in <code>chol</code>, odds of being female increase by a factor of 0.992 (p=0.001).</p>
<p>The accuracy is 0.706, sensitivity is 0.99, the specificity is 0.09, the precision is 0.702, and the AUC is 0.597. Accuracy and precision are not ideal, as they are both pretty low. The sensitivity is higher than the specificity, meaning that the model is predicting true values better than false values. The AUC is 0.597, which is classified as bad. Our AUC makes sense when looking at the ROC curve, which is nearly linear. When looking at our density plot of the log-odds, the gray area represents mis-classification, and there is a lot of gray. Our AUC/ROC curve as well as our density plot show that this is not the best model.</p>
</div>
<div id="logistic-regression-w-the-rest-of-variables" class="section level2">
<h2>6. Logistic Regression w/ the rest of variables</h2>
<pre class="r"><code>#Model 
data2 &lt;- data %&gt;% select(-sex2) %&gt;% select(-age_c) %&gt;% select(-logit)
fit &lt;- glm(sex ~., data=data2, family=&quot;binomial&quot;)
probs &lt;- predict(fit, type=&quot;response&quot;) 
#Confusion Matrix 
table(predict=as.numeric(probs&gt;.5),truth=data$sex)%&gt;%addmargins  </code></pre>
<pre><code>##        truth
## predict   0   1 Sum
##     0    56  29  85
##     1    40 178 218
##     Sum  96 207 303</code></pre>
<pre class="r"><code>#In-sample classification diagnostics
class_diag(probs, data2$sex) </code></pre>
<pre><code>##         acc      sens      spec       ppv        f1       auc
## 1 0.7722772 0.8599034 0.5833333 0.8165138 0.8376471 0.8369565</code></pre>
<pre class="r"><code>#10 fold CV 
k=10
data &lt;- data2 %&gt;% sample_frac 
folds &lt;- ntile(1:nrow(data),n=10) 

diags&lt;-NULL
set.seed(1234)  
for(i in 1:k){
  train &lt;- data[folds!=i,] 
  test &lt;- data[folds==i,] 
  truth &lt;- test$sex 
  
  fit &lt;- glm(sex~., data=train, family=&quot;binomial&quot;)
  probs &lt;- predict(fit, newdata=test, type=&quot;response&quot;)
  
  diags&lt;-rbind(diags,class_diag(probs,truth))
}

summarize_all(diags,mean) </code></pre>
<pre><code>##         acc      sens      spec       ppv        f1       auc
## 1 0.7423656 0.8352375 0.5582945 0.7959968 0.8118402 0.7765656</code></pre>
<pre class="r"><code>#LASSO 
y &lt;- as.matrix(data2$sex) 
preds &lt;- model.matrix(sex~., data=data2) [,-1] 
preds &lt;- scale(preds)  
cv &lt;- cv.glmnet(preds, y, family=&quot;binomial&quot;) 
lasso_fit&lt;-glmnet(preds,y,family=&quot;binomial&quot;,lambda=cv$lambda.1se) 
coef(lasso_fit) </code></pre>
<pre><code>## 22 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                      s0
## (Intercept)  0.85695106
## age         -0.04977644
## cp1          .         
## cp2          .         
## cp3          .         
## trestbps     .         
## chol        -0.22345851
## fbs1         .         
## restecg      .         
## max          .         
## exang1       .         
## oldpeak      .         
## slope1       .         
## slope2       .         
## ca1          .         
## ca2          .         
## ca3          .         
## ca4          .         
## thal1        .         
## thal2       -0.57172328
## thal3        .         
## target1     -0.14844979</code></pre>
<pre class="r"><code>#10 fold CV w/ LASSO variables 
data3 &lt;- data2 %&gt;% mutate(thal2 = ifelse(data2$thal==2, 1, 0))
data &lt;- data3 %&gt;% sample_frac 
folds &lt;- ntile(1:nrow(data),n=10) 
diags&lt;-NULL
set.seed(1234)  
for(i in 1:k){
  train &lt;- data[folds!=i,] 
  test &lt;- data[folds==i,] 
  truth &lt;- test$sex 
  
  fit &lt;- glm(sex~age + chol+thal2+target, 
             data=train, family=&quot;binomial&quot;)
  probs &lt;- predict(fit, newdata=test, type=&quot;response&quot;)
  
  diags&lt;-rbind(diags,class_diag(probs,truth))
}
diags%&gt;%summarize_all(mean)</code></pre>
<pre><code>##         acc      sens      spec       ppv        f1       auc
## 1 0.7170968 0.8504828 0.4441056 0.7682144 0.8018094 0.7827306</code></pre>
<p>For the normal-theory model, the accuracy is 0.77, the sensitivity is 0.86, the specificity is 0.58, the precision is 0.82, and the AUC is 0.84. Accuracy and precision are not ideal, but are a little higher than the model from the previous question. The sensitivity is higher than the specificity, which indicates that this model is better at predicting true values than false ones. The AUC is better than the previous question and is now classified as good.</p>
<p>When performing a 10-fold CV with the same model, the accuracy is 0.75, sensitivity is 0.85, specificity is 0.56, precision is 0.80, and the AUC is 0.78. Accuracy, precision, specificity, and sensitivity all decrease when compared to the previous model. AUC decreases from the previous model as well, which is a sign of over-fitting. The AUC in this model is considered fair.</p>
<p>LASSO was then performed to determine which coefficient estimates are non-zero, which were <code>age</code>, <code>chol</code>, <code>thal</code> (level 2), and <code>target</code>. We recoded for the <code>thal</code> level 2 and created a binary variable called <code>thal2</code>, where 1 = <code>thal</code> was 2, and 0 = <code>thal</code> was anything but 2. Using these retained variables we performed a 10-fold CV. The AUC of this model was 0.783, which is classified as fair. Based on all of these models, the best was our first model (normal-theory), since this has the highest AUC (0.84).</p>
</div>
